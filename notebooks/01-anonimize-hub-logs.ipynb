{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymize JupyterHub Logs\n",
    "\n",
    "This notebooks extracts anonymized, publishable user session information from JupyterHub logs.\n",
    "\n",
    "## Extract user session information from the log\n",
    "\n",
    "We only care about server starts & stops, so we extract lines related to this from the JupyterHub log. We might pre-filter the log with something like `grep 'seconds to' jupyterhub.log > filtered-jupyterhub.log` to make processing faster - the Berkeley JupyterHub logfile for Spring 2018 semester was 7G without this pre-filtering!\n",
    "\n",
    "## Anonymize user names\n",
    "\n",
    "User names should not leak, but we want to establish usage patterns for individual users across time. We accomplish this by hashing each username with an ephemeral secret salt. This ensures user names stay same across each run of this notebook, but can't be co-related with other datasets that might be made in the future.\n",
    "\n",
    "## Reduce data resolution\n",
    "\n",
    "User activity timestamps are important for most analysis, but can also be used in attacks to de-anonymize users. To safeguard against this, we only provide timestamps with hourly resolution. This is good enough for most analysis at large scales.\n",
    "\n",
    "## Eliminate periods of low activity \n",
    "\n",
    "If only a small number of users activities happen in any given hour, the risk of them being de-anonymized becomes higher. For example, if you know student A was active on the hub at Friday Jan 21 2018 9PM via other channels (maybe they tweeted about it!), then from this dataset you can find their hashed user id & hence track their activity across time! We try to make this harder by eliminating data for the hours where less than `k` user activities happened. We set `k` to 5 by default.\n",
    "\n",
    "## Ongoing process\n",
    "\n",
    "This is a best effort in anonymizing usage data, and could use improvements! If you think of any, please let me know!\n",
    "\n",
    "## Further reading\n",
    "\n",
    "The wikipedia article for [k-anonymity](https://en.wikipedia.org/wiki/K-anonymity) is pretty good. The [original paper](https://dataprivacylab.org/dataprivacy/projects/kanonymity/paper3.pdf) is also fairly readable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import hmac\n",
    "import json\n",
    "import dateutil\n",
    "import os\n",
    "import secrets\n",
    "\n",
    "# Generate a HMAC key for salting the username\n",
    "# This is only kept in memory, so we can not reverse this after this process dies\n",
    "HMAC_KEY = secrets.token_bytes(32)\n",
    "\n",
    "def parse_activity_line(line):\n",
    "    \"\"\"\n",
    "    Parses a user server start/stop line from JupyterHub logs\n",
    "    \n",
    "    Returns a tuple of (timestamp, anonymized_username, action).\n",
    "    \n",
    "    timestamp is rounded out to the nearest hour for anonymization purposes.\n",
    "    \"\"\"\n",
    "    log_entry = json.loads(line)\n",
    "    payload = log_entry['textPayload']\n",
    "    parts = payload.split() # this is...  terrible and error prone.\n",
    "    try:\n",
    "        spawn_time = ''\n",
    "        hub = log_entry['labels']['k8s-pod/release']\n",
    "        # Round all timestamp info to the hour to make it more anonymous\n",
    "        ts = dateutil.parser.parse('{} {}'.format(parts[1], parts[2])).replace(minute=0, second=0, microsecond=0)\n",
    "        ts_true = dateutil.parser.parse('{} {}'.format(parts[1], parts[2]))\n",
    "        user = parts[6].strip()\n",
    "        userhash = hmac.new(HMAC_KEY, user.encode(), hashlib.sha512).hexdigest()\n",
    "        if 'start' in parts[-1]:\n",
    "            action = 'start'\n",
    "            spawn_time = parts[8]\n",
    "        else:\n",
    "            action = 'stop'\n",
    "    except IndexError:\n",
    "        # Poor person's debugger!\n",
    "        print(log_entry, parts)\n",
    "        raise\n",
    "    return (ts, ts_true, userhash, action, hub, spawn_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session_data(infile_path, outfile_path, min_entries_per_hour=5):\n",
    "    \"\"\"\n",
    "    Generate user session data from JupyterHub logs in infile_path\n",
    "    \n",
    "    min_entries_per_hour is the minimum number of activity entries that must\n",
    "    be present in each hour for the hour to be included in the output.\n",
    "    \"\"\"\n",
    "    with open(infile_path) as infile, open(outfile_path, 'w') as outfile:\n",
    "        current_hour_entries = []\n",
    "        last_hour = None\n",
    "        for l in infile:\n",
    "            if 'seconds to' in l:\n",
    "                timestamp, timestamp_true, user, action, hub, spawn_time = parse_activity_line(l)\n",
    "                if last_hour is None:\n",
    "                    last_hour = timestamp\n",
    "                if timestamp == last_hour:\n",
    "                    current_hour_entries.append(json.dumps({'timestamp': timestamp_true.isoformat(), 'user': user, 'action': action, 'hub': hub, 'spawn_time': spawn_time}))\n",
    "                else:\n",
    "                    if len(current_hour_entries) >= min_entries_per_hour:\n",
    "                        outfile.write('\\n'.join(current_hour_entries) + '\\n')\n",
    "                    else:\n",
    "                        print(f'Skipped entry for {timestamp}: had less than {min_entries_per_hour} actions')\n",
    "                    last_hour = timestamp\n",
    "                    current_hour_entries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate usage data for Spring 2023\n",
    "generate_session_data(\n",
    "    '../spring-2023/user-sessions-spring-2023.jsonl', \n",
    "    '../spring-2023/user-starts-stops-spring-2023-09-27.jsonl',\n",
    "    5\n",
    "    )\n",
    "##root_dir = '/e/datahub/'\n",
    "#months = ['01', '02', '03', '04', '05']\n",
    "#months = [root_dir+m for m in months]\n",
    "#for month in months:\n",
    "#    days = os.listdir(month)\n",
    "#    days = [month+'/'+d for d in days]\n",
    "#    for day in days:\n",
    "#        usage_data = os.listdir(day)\n",
    "#        usage_data = [day+'/'+u for u in usage_data]\n",
    "#        for usage_hour in usage_data:\n",
    "#            print('parsing: ' + usage_hour)\n",
    "#            generate_session_data(\n",
    "#                usage_hour, \n",
    "#                '/home/sknapp/src/datahub-usage-analysis/data/processed/user-starts-stops-spring-2023.jsonl',\n",
    "#                5\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-anonimize-hub-logs.ipynb\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
