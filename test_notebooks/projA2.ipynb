{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"projA2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0f9b2de18190d9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Project A2: Predicting Housing Prices in Cook County\n",
    "\n",
    "## Due Date: Thursday, March 21st, 11:59 PM\n",
    "\n",
    "You must submit this assignment to Gradescope by the on-time deadline, Thursday, March 21st, 11:59 PM. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted. **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to reach out to staff for submission support. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to last-minute requests for assistance (TAs need to sleep, after all!).\n",
    "\n",
    "Please read the instructions carefully when submitting your work to Gradescope. \n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the project, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the collaborators cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Project A1, you performed some basic Exploratory Data Analysis (EDA), laying out the thought process that leads to certain modeling decisions. Then, you added a few new features to the dataset and cleaned the data in the process.\n",
    "\n",
    "In this project, you will specify and fit a linear model to a few features of the housing data to predict house prices. Next, we will analyze the error of the model and brainstorm ways to improve the model's performance. Finally, we'll delve deeper into the implications of predictive modeling within the Cook County Assessor's Office (CCAO) case study, especially because statistical modeling is how the CCAO valuates properties. Given the history of racial discrimination in housing policy and property taxation in Cook County, consider the impacts of your modeling results as you work through this project, and think about what fairness might mean to property owners in Cook County.\n",
    "\n",
    "After this part of the project, you should be comfortable with:\n",
    "- Implementing a data processing pipeline using `pandas`.\n",
    "- Using `scikit-learn` to build and fit linear models.\n",
    "\n",
    "## Score Breakdown\n",
    "\n",
    "Question | Manual | Points\n",
    "----|----|----\n",
    "1a | Yes | 1\n",
    "1b | Yes | 1\n",
    "1c | No | 1\n",
    "1d | Yes | 1\n",
    "1e | Yes | 1\n",
    "2 | No | 2\n",
    "3a | No | 2\n",
    "3b | No | 3\n",
    "3c | No | 2\n",
    "4a | Yes | 2\n",
    "4b | No | 1\n",
    "5a | No | 0\n",
    "5b | No | 0\n",
    "5c | No | 0\n",
    "5d | No | 3\n",
    "5e | No | 0\n",
    "5f | No | 0\n",
    "5g | No | 0\n",
    "6a | No | 1\n",
    "6b | No | 2\n",
    "6c | Yes | 2\n",
    "7a | Yes | 1\n",
    "7b | Yes | 2\n",
    "Test Prediction | No | 3\n",
    "Total | 8 | 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question. \n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder.** Please be sure to check your results carefully.\n",
    "\n",
    "### Debugging Guide\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, common `pandas` errors, RegEx, visualizations, and Proj. A1 and A2 common questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62cfd21463535cac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from ds100_utils import *\n",
    "from feature_func import *\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('cook_county_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is split into a training set, a validation set, and a test set. Importantly, the test set does not contain values for our target variable, `Sale Price`. In this project, you will train a model on the training and validation sets and then use this model to predict the `Sale Price`s of the test set. In the cell below, we load the training and validation sets into the `DataFrame` `training_val_data` and the test set into the `DataFrame` `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8fea30adc9d489b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_val_data = pd.read_csv(\"cook_county_train_val.csv\", index_col='Unnamed: 0')\n",
    "test_data = pd.read_csv(\"cook_county_contest_test.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d6d509b6e854e10",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As a good sanity check, we should at least verify that the shape of the data matches the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c841a2de55691502",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 204792 observations and 62 features in training data\n",
    "assert training_val_data.shape == (204792, 62)\n",
    "# 55311 observations and 61 features in test data\n",
    "assert test_data.shape == (55311, 61)\n",
    "# Sale Price is provided in the training/validation data\n",
    "assert 'Sale Price' in training_val_data.columns.values\n",
    "# Sale Price is hidden in the test data\n",
    "assert 'Sale Price' not in test_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce9acc2f62c96e59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's remind ourselves of the data available to us in the Cook County dataset. Remember, a more detailed description of each variable is included in `codebook.txt`, which is in the same directory as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e60a7a0cda5eecf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_val_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Human Context and Ethics\n",
    "\n",
    "In this part of the project, we will explore the human context of our housing dataset. **You should watch [Lecture 15](https://ds100.org/sp24/lecture/lec15/) before attempting this question.**\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 1a\n",
    "\"How much is a house worth?\" Who might be interested in an answer to this question? **Please list at least three different parties (people or organizations) and state whether each one has an interest in seeing the housing price be low or high.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 1b\n",
    "\n",
    "Which of the following scenarios strike you as unfair, and why? You can choose more than one. There is no single right answer, but you must explain your reasoning. Would you consider some of these scenarios more (or less) fair than others? Why?\n",
    "\n",
    "A. A homeowner whose home is assessed at a higher price than it would sell for.  \n",
    "B. A homeowner whose home is assessed at a lower price than it would sell for.  \n",
    "C. An assessment process that systematically overvalues inexpensive properties and undervalues expensive properties.  \n",
    "D. An assessment process that systematically undervalues inexpensive properties and overvalues expensive properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c\n",
    "\n",
    "Consider a model that is fit to $n = 50$ training observations. We denote the response as $y$ (Log Sale Price), the prediction as $\\hat{y}$, and the corresponding residual to be $y - \\hat{y}$. Which residual plot corresponds to a model that might make property assessments that result in regressive taxation? Recall from Lecture 15 that regressive taxation overvalues inexpensive properties and undervalues expensive properties. Assume that all three plots use the same vertical scale and that the horizontal line marks $y - \\hat{y} = 0$. Assign `q1c` to the string letter corresponding to your plot choice.\n",
    "\n",
    "**Hint:** When a model overvalues a property (predicts a `Sale Price` greater than the actual `Sale Price`), what are the relative sizes of $y$ and $\\hat{y}$? What about when a model undervalues a property?\n",
    "\n",
    "<img src='images/res_plots.png' width=\"900px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1c = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CCAO Dataset\n",
    "\n",
    "You'll work with the dataset from the Cook County Assessor's Office (CCAO) in Illinois. This government institution determines property taxes across most of Chicago's metropolitan areas and nearby suburbs. In the United States, all property owners must pay property taxes, which are then used to fund public services, including education, road maintenance, and sanitation. These property tax assessments are based on property values estimated using statistical models considering multiple factors, such as real estate value and construction cost.\n",
    "\n",
    "However, this system is not without flaws. In late 2017, a lawsuit was filed against the office of Cook County Assessor Joseph Berrios for producing \"[racially discriminatory assessments and taxes](https://www.chicagotribune.com/politics/ct-cook-county-board-assessor-berrios-met-20170718-story.html).\" The lawsuit included claims that the assessor's office undervalued high-priced homes and overvalued low-priced homes, creating a visible divide along racial lines. Wealthy homeowners, who were typically white, [paid less in property taxes](https://fix8media-chicago.squarespace.com/bpnc-v-berrios-resource-page), whereas [working-class, non-white homeowners paid more](https://www.chicagotribune.com/news/breaking/ct-cook-county-assessor-berrios-sued-met-20171214-story.html).\n",
    "\n",
    "The Chicago Tribune's four-part series, \"[The Tax Divide](https://www.chicagotribune.com/investigations/ct-tax-divide-investigation-20180425-storygallery.html),\" delves into how this was uncovered. After \"compiling and analyzing more than 100 million property tax records from the years 2003 through 2015, along with thousands of pages of documents, then vetting the findings with top experts in the field,\" they discovered that \"residential assessments had been so far off the mark for so many years.\" You can read more about their investigation [here](https://apps.chicagotribune.com/news/watchdog/cook-county-property-tax-divide/assessments.html).\n",
    "\n",
    "Make sure to watch [Lecture 15](https://ds100.org/sp24/lecture/lec15/) before answering the following questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 1d\n",
    "\n",
    "What were the central problems with the earlier property tax system in Cook County as reported by the Chicago Tribune? What were the primary causes of these problems? \n",
    "\n",
    "**Note:** Along with reading the paragraph above, you will need to watch [Lecture 15](https://ds100.org/sp24/lecture/lec15/) to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 1e\n",
    "\n",
    "In addition to being regressive, how did the property tax system in Cook County place a disproportionate tax burden on non-white property owners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Preparing Data\n",
    "\n",
    "\n",
    "Let's split the dataset into a training set and a validation set. We will use the training set to fit our model's parameters and the validation set to evaluate how well our model will perform on unseen data drawn from the same distribution. If we used all the data to fit our model, we would not have a way to estimate model performance on **unseen data** such as the test set in `cook_county_contest_test.csv`.\n",
    "\n",
    "In the cell below, complete the function `train_val_split` that splits `data` into two smaller `DataFrame`s named `train` and `validation`. Let `train` contain 80% of the data, and let `validation` contain the remaining 20%. **You should not import any additional libraries for this question.** \n",
    "\n",
    "You should only use `NumPy` functions to generate randomness! Your answer should use the variable `shuffled_indices` defined for you. Take a look at the [documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html) for `np.permutation` to better understand what `shuffled_indices` contains.\n",
    "\n",
    "**Hint:** While there are multiple solutions, one way is to create two `NumPy` arrays named `train_indices` and `validation_indices` (or any variable names of your choice) that contain a *random* 80% and 20% of the indices, respectively. Then, use these arrays to index into `data` to create your final `train` and `validation` `DataFrame`s. To ensure that your code matches the solution, use the first 80% as the training set and the last 20% as the validation set. Remember, the values you use to partition `data` must be integers!\n",
    "\n",
    "*The provided tests check that you not only answered correctly but ended up with the same train/validation split as our reference implementation. Testing later on is easier this way.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This makes the train-validation split in this section reproducible across different runs \n",
    "# of the notebook. You do not need this line to run train_val_split in general.\n",
    "\n",
    "# DO NOT CHANGE THIS LINE\n",
    "np.random.seed(1337)\n",
    "# DO NOT CHANGE THIS LINE\n",
    "\n",
    "def train_val_split(data):\n",
    "    \"\"\" \n",
    "    Takes in a DataFrame `data` and randomly splits it into two smaller DataFrames \n",
    "    named `train` and `validation` with 80% and 20% of the data, respectively. \n",
    "    \"\"\"\n",
    "    \n",
    "    data_len = data.shape[0]\n",
    "    shuffled_indices = np.random.permutation(data_len)\n",
    "    ...\n",
    "    train = ...\n",
    "    validation = ...\n",
    "   \n",
    "    return train, validation\n",
    "train, validation = train_val_split(training_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Fitting a Simple Model\n",
    "\n",
    "Let's fit our linear regression model using the ordinary least squares estimator! We will start with something simple by using only two features: the **number of bedrooms** in the household and the **log-transformed total area covered by the building** (in square feet). \n",
    "\n",
    "Consider the following expression for our first linear model that contains one of the features:\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms})\n",
    "$$\n",
    "\n",
    "In parallel, we will also consider a second model that contains both features:\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms}) + \\theta_2 \\cdot (\\text{Log Building Square Feet})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 3a\n",
    "\n",
    "**Without running any calculation or code**, assign `q3a` to be the comparator ('>=', '=', '<=') that fills the blank in the following statement:\n",
    "\n",
    "We quantify the loss on our linear models using MSE (Mean Squared Error). Consider the training loss of the first model and the training loss of the second model. We are guaranteed that:\n",
    "\n",
    "$$\n",
    "\\text{Training Loss of the 1st Model} ~~  \\_\\_\\_\\_ ~~ \\text{Training Loss of the 2nd Model}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q3a = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pipeline Function\n",
    "\n",
    "In Project A1, you wrote a few functions that added features to the dataset. Instead of calling them manually one by one each time, it is best practice to encapsulate all of this feature engineering into one \"pipeline\" function. Defining and using a pipeline reduces all the feature engineering to just one function call and ensures that the same transformations are applied to all data.  Below, we combined some functions into a single helper function that outputs `X` and `Y` for the first model above. Try to understand what this function does! \n",
    "\n",
    "**Note 1:** We have automatically imported staff implementations of the functions you wrote in Project A1. These functions are `remove_outliers`, `add_total_bedrooms`, `find_expensive_neighborhoods`, `add_in_expensive_neighborhood`, and `ohe_roof_material`. You are welcome to copy over your own implementations if you would like.\n",
    "\n",
    "**Note 2:** The staff implementation provided for `remove_outliers` is slightly different from what you did in Project A1. Here `remove_outliers` is exclusive for the bounds whereas in Project A1, it was inclusive for the bounds. `remove_outliers` will only output values strictly greater than the lower bound and strictly smaller than the upper bound. Feel free to still use your original implementation of the function; it shouldn't affect your score if it was done correctly but may slightly change your approach to `q5f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_func import *    # Import functions from Project A1\n",
    "\n",
    "###### Copy any function you would like to below ######\n",
    "...\n",
    "#######################################################\n",
    "\n",
    "\n",
    "def feature_engine_simple(data):\n",
    "    # Remove outliers\n",
    "    data = remove_outliers(data, 'Sale Price', lower=499)\n",
    "    # Create Log Sale Price column\n",
    "    data = log_transform(data, 'Sale Price')\n",
    "    # Create Bedroom column\n",
    "    data = add_total_bedrooms(data)\n",
    "    # Select X and Y from the full data\n",
    "    X = data[['Bedrooms']]\n",
    "    Y = data['Log Sale Price']\n",
    "    return X, Y\n",
    "\n",
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Process the data using the pipeline for the first model.\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "X_train_m1_simple, Y_train_m1_simple = feature_engine_simple(train_m1)\n",
    "X_valid_m1_simple, Y_valid_m1_simple = feature_engine_simple(valid_m1)\n",
    "\n",
    "# Take a look at the result\n",
    "display(X_train_m1_simple.head())\n",
    "display(Y_train_m1_simple.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.pipe`\n",
    "\n",
    "Alternatively, we can build the pipeline using `pd.DataFrame.pipe` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html)). Take a look at our use of `pd.DataFrame.pipe` below. \n",
    "\n",
    "The following function `feature_engine_pipe` takes in a `DataFrame` `data`, a list `pipeline_functions` containing 3-element tuples `(function, arguments, keyword_arguments)` that will be called on `data` in the pipeline, and the label `prediction_col` that represents the column of our target variable (`Sale Price` in this case). You can use this function with each of the tuples passed in through `pipeline_functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define feature_engine_pipe; no further action is needed.\n",
    "def feature_engine_pipe(data, pipeline_functions, prediction_col):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    for function, arguments, keyword_arguments in pipeline_functions:\n",
    "        if keyword_arguments and (not arguments):\n",
    "            data = data.pipe(function, **keyword_arguments)\n",
    "        elif (not keyword_arguments) and (arguments):\n",
    "            data = data.pipe(function, *arguments)\n",
    "        else:\n",
    "            data = data.pipe(function)\n",
    "    X = data.drop(columns=[prediction_col])\n",
    "    Y = data.loc[:, prediction_col]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 3b\n",
    "\n",
    "It is time to prepare the training and validation data for the two models we proposed above. Use the following two cells to reload a fresh dataset from scratch and run them through the following preprocessing steps using `feature_engine_pipe` for each model:\n",
    "\n",
    "- Perform a `train_val_split` on the original dataset, loaded as the `DataFrame` `full_data`. Let 80% of the set be training data, and 20% of the set be validation data. \n",
    "- For both the training and validation set,\n",
    "    1. Remove outliers in `Sale Price` so that we consider households with a price that is greater than 499 dollars (or equivalently, a price that is 500 dollars or greater). \n",
    "    2. Apply log transformations to the `Sale Price` and the `Building Square Feet` columns to create two new columns, `Log Sale Price` and `Log Building Square Feet`.\n",
    "    3. Extract the total number of bedrooms into a new column `Bedrooms` from the `Description` column.\n",
    "    4. Select the columns `Log Sale Price` and `Bedrooms` (and `Log Building Square Feet` if this is the second model). We have implemented the helper function `select_columns` for you.\n",
    "    5. Return the design matrix $\\mathbb{X}$ and the observed vector $\\mathbb{Y}$. Note that $\\mathbb{Y}$ refers to the transformed `Log Sale Price`, not the original `Sale Price`. **Your design matrix and observed vector should be `NumPy` arrays or `pandas` `DataFrame`s**.\n",
    "\n",
    "Assign the final training data and validation data for both models to the following set of variables:\n",
    "\n",
    "- First Model: `X_train_m1`, `Y_train_m1`, `X_valid_m1`, `Y_valid_m1`. This is already implemented for you. \n",
    "- Second Model: `X_train_m2`, `Y_train_m2`, `X_valid_m2`, `Y_valid_m2`. Please implement this in the second cell below. You may use the first model as an example.\n",
    "\n",
    "For an example of how to work with pipelines, we have processed model 1 for you using `m1_pipelines` by passing in the corresponding pipeline functions as a list of tuples in the below cell. Your task is to do the same for model 2 in the cell after —— that is, save your pipeline functions as a list of tuples and assign it to `m2_pipelines` for model 2.\n",
    "\n",
    "As a refresher, the equations model 1 and model 2, respectively, are:\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms}) + \\theta_2 \\cdot (\\text{Log Building Square Feet})\n",
    "$$\n",
    "\n",
    "**Note**: Do not change the line `np.random.seed(1337)` as it ensures we are partitioning the dataset the same way for both models (otherwise, their performance isn't directly comparable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Apply feature engineering to the data using the pipeline for the first model\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "\n",
    "# Helper function\n",
    "def select_columns(data, *columns):\n",
    "    \"\"\"Select only columns passed as arguments.\"\"\"\n",
    "    return data.loc[:, columns]\n",
    "\n",
    "# Pipelines, a list of tuples\n",
    "m1_pipelines = [\n",
    "    (remove_outliers, None, {\n",
    "        'variable': 'Sale Price',\n",
    "        'lower': 499,\n",
    "    }),\n",
    "    (log_transform, None, {'col': 'Sale Price'}),\n",
    "    (add_total_bedrooms, None, None),\n",
    "    (select_columns, ['Log Sale Price', 'Bedrooms'], None)\n",
    "]\n",
    "\n",
    "X_train_m1, Y_train_m1 = feature_engine_pipe(train_m1, m1_pipelines, 'Log Sale Price')\n",
    "X_valid_m1, Y_valid_m1 = feature_engine_pipe(valid_m1, m1_pipelines, 'Log Sale Price')\n",
    "\n",
    "# Take a look at the result\n",
    "# It should be the same above as the result returned by feature_engine_simple\n",
    "display(X_train_m1.head())\n",
    "display(Y_train_m1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS LINE\n",
    "np.random.seed(1337)\n",
    "# DO NOT CHANGE THIS LINE\n",
    "\n",
    "# Process the data using the pipeline for the second model\n",
    "train_m2, valid_m2 = ...\n",
    "\n",
    "m2_pipelines = ...\n",
    "\n",
    "X_train_m2, Y_train_m2 = ...\n",
    "X_valid_m2, Y_valid_m2 = ...\n",
    "\n",
    "\n",
    "# Take a look at the result\n",
    "display(X_train_m2.head())\n",
    "display(Y_train_m2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 3c\n",
    "\n",
    "Finally, let's do some regression!\n",
    "\n",
    "We first initialize a `sklearn.linear_model.LinearRegression` object [(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) for both of our models. We set the `fit_intercept = True` to ensure that the linear model has a non-zero intercept (i.e., a bias term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_m1 = lm.LinearRegression(fit_intercept=True)\n",
    "linear_model_m2 = lm.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now it's time to fit our linear regression model. Use the cell below to fit both models and then use it to compute the fitted values of `Log Sale Price` over the training data and the predicted values of `Log Sale Price` for the validation data.\n",
    "\n",
    "Assign the predicted values from both of your models on the training and validation set to the following variables:\n",
    "\n",
    "- First Model: predicted values on **training set**: `Y_fitted_m1`, predicted values on **validation set**: `Y_predicted_m1`\n",
    "- Second Model: predicted values on **training set**: `Y_fitted_m2`, predicted values on **validation set**: `Y_predicted_m2`\n",
    "\n",
    "**Note**: To make sure you understand how to find the predicted value for both the training and validation data set, there won't be any hidden tests for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the 1st model\n",
    "...\n",
    "# Compute the fitted and predicted values of Log Sale Price for 1st model\n",
    "Y_fitted_m1 = ...\n",
    "Y_predicted_m1 = ...\n",
    "\n",
    "# Fit the 2nd model\n",
    "...\n",
    "# Compute the fitted and predicted values of Log Sale Price for 2nd model\n",
    "Y_fitted_m2 = ...\n",
    "Y_predicted_m2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Evaluate Our Simple Model\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "Let's now move into the analysis of our two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values.\n",
    "    Input:\n",
    "      predicted (1D array): Vector of predicted/fitted values\n",
    "      actual (1D array): Vector of actual values\n",
    "    Output:\n",
    "      A float, the RMSE value.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 4a\n",
    "\n",
    "One way of understanding a model's performance (and appropriateness) is through a plot of the residuals versus the observations.\n",
    "\n",
    "In the cell below, use `plt.scatter` [(documentation)](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) to plot the residuals from predicting `Log Sale Price` using **only the second model** against the original `Log Sale Price` for the **validation data**. With such a large dataset, it is difficult to avoid overplotting entirely. You should also **ensure that the dot size and opacity in the scatter plot are set appropriately** to reduce the impact of overplotting as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d79f42d60b94fca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 4b\n",
    "\n",
    "Based on the structure you see in your plot, does this model seem like it will correspond to _regressive_, _fair_, or _progressive_ taxation?\n",
    "\n",
    "Assign \"regressive\", \"fair\" or \"progressive\" to `q4b` in the cell below accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4b = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our simple model explains some of the variability in price, there is certainly still a lot of room for improvement —— one reason is we have been only utilizing 1 or 2 features (out of a total of 70+) so far! Can you engineer and incorporate more features to improve the model's fairness and accuracy? We won't be asking you to provide your answers here, but this will be important going into the next part of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 5\n",
    "\n",
    "It is time to build your own model!\n",
    "\n",
    "You will conduct feature engineering on your training data using the `feature_engine_final` function (you will define this in `q5d`), fit the model with this training data, and compute the training Root Mean Squared Error (RMSE). Then, we will process our test data with `feature_engine_final`, use the model to predict `Log Sale Price` for the test data, transform the predicted and original log values back into their original forms (by using `delog`), and compute the test RMSE.\n",
    "\n",
    "Your goal in Question 5 is to:\n",
    "\n",
    "* Define a function to perform feature engineering and produce a design matrix for modeling.\n",
    "* Apply this feature engineering function to the training data and use it to train a model that can predict the `Log Sale Price` of houses.\n",
    "* Use this trained model to predict the `Log Sale Price`s of the test set. Remember that our test set does not contain the true `Sale Price` of each house –— your model is trying to guess them! \n",
    "* Submit your predicted `Log Sale Price`s on the test set to Gradescope.\n",
    "\n",
    "\n",
    "Right under the grading scheme, we will outline some important Datahub logistics. **Please make sure you read this carefully to avoid running into memory issues later!**\n",
    "\n",
    "* In Question 5a, you can explore possible features for your model. This portion is **not graded**.\n",
    "* In Question 5b, you can perform EDA on the dataset. This portion is **not graded**.\n",
    "* In Question 5c, you can define feature engineering helper functions. This portion is **not graded**.\n",
    "* In Question 5d, you will create your design matrix and train a model. This portion is **is graded**.\n",
    "* In Question 5e, you can fit and evaluate your model. This portion is **not graded**.\n",
    "* In Question 5f, you will generate the predictions for the test set. This portion is **is graded**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Scheme\n",
    "\n",
    "Your grade for Question 5 will be based on your model's RMSE when making predictions on the training set, as well as your model’s RMSE when making predictions on the test set. The tables below provide scoring guidelines. If your RMSE lies in a particular range, you will receive the number of points associated with that range.\n",
    "\n",
    "**Important**: while your training RMSE can be checked at any time in this notebook, your test RMSE can only be checked by submitting your model’s predictions to Gradescope. **You will only be able to submit your test set predictions to Gradescope up to 4 times per day**. Attempts will not carry over across days, so we recommend planning ahead to make sure you have enough time to finetune your model! \n",
    "\n",
    "The thresholds are as follows:\n",
    "\n",
    "Points | 3 | 2 | 1 | 0\n",
    "--- | --- | --- | --- | ---\n",
    "Training RMSE | Less than 200k | [200k, 240k) | [240k, 280k) | More than 280k\n",
    "\n",
    "Points | 3 | 2 | 1 | 0\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Less than 240k | [240k, 280k) | [280k, 300k) | More than 300k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Some notes before you start\n",
    "\n",
    "- **If you are running into memory issues, restart the kernel and only run the cells you need to.** The cell below (question cell) contains most to all of the imports necessary to successfully complete this portion of the project, so it can be completed independently code-wise from the remainder of the project, and you do not need to rerun the cell at the top of this notebook. The autograder will have more than 4GB of memory, so you will not lose credit as long as your solution to Question 5 is within the total memory (4GB) limits of Datahub. By default, we reset the memory and clear all variables using `%reset -f`. If you want to delete specific variables, you may also use `del` in place of `%reset -f%`. For example, the following code will free up memory from data used for older models: `del training_val_data, test_data, train, validation, X_train_m1, X_valid_m1, X_train_m2, X_valid_m1`. Our staff solution can be run independently from all other questions, so we encourage you to do the same to make debugging easier.\n",
    "- **If you need the data again after deleting the variables or resetting, you must reload them again from earlier in the notebook.**\n",
    "- You will be predicting `Log Sale Price` on the data stored in `cook_county_contest_test.csv`. We will delog/exponentiate your prediction on Gradescope to compute RMSE and use this to score your model. Before submitting to Gradescope, make sure that your predicted values can all be delogged (i.e., if one of your `Log Sale Price` predictions is 60, it is too large; $e^{60}$ is too big!)\n",
    "- You MUST remove any additional new cells you add before submitting to Gradescope to avoid any autograder errors. \n",
    "- **You can only submit your test set prediction CSV file to Gradescope up to 4 times per day. Start early!** In the case that you are approved for an extension, you will be granted 4 more submissions for each day the deadline has been extended.\n",
    "\n",
    "**PLEASE READ THE ABOVE MESSAGE CAREFULLY!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 lines below to clean up memory from previous questions and reinitialize Otter!\n",
    "# If you want to refer to any functions or variables you defined at any point earlier in the project, \n",
    "# Place them in the cell under Question 5c so that you can access them after the memory is reset.\n",
    "# If you think you will not run into any memory issues, you are free to comment out the next 3 lines as well.\n",
    "\n",
    "%reset -f                               \n",
    "import otter                            \n",
    "grader = otter.Notebook(\"projA2.ipynb\")\n",
    "\n",
    "# Imports all the necessary libraries again\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from ds100_utils import *\n",
    "from feature_func import *\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5a: Finding Potential Features\n",
    "\n",
    "**This question is not graded** – it is intended to give helpful guidance on how to get started with feature engineering in `q5d`. You may write as little or as much as you would like here; it will not factor into your grade. Read the documentation about the dataset in `codebook.txt`, located in this directory. Is there any data you think may be related to housing prices? Include them below for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5b: More EDA\n",
    "\n",
    "\n",
    "**This question is not graded** – it is intended to give helpful guidance on how to get started with feature engineering. You may write as little or as much as you would like here; it will not factor into your grade. Use the scratch space below to conduct any additional EDA you would like to see. You may use this space to make additional plots to help you visualize the relationship between any variables or compute any relevant statistics. You are free to add any number of cells as needed below and before the next question. You may find it helpful to review Project A1 and the techniques we explore there.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>[<b>Click to Expand</b>] Some potential ideas. </summary>\n",
    "    \n",
    "* Plot the distribution of a variable. Is this variable heavily skewed? Are there any outliers? This can inform how you engineer your features later on.\n",
    "\n",
    "* Make a scatter plot between a continuous feature and the outcome. Is there a relationship? Is there a transformation that may linearize the relationship?\n",
    "\n",
    "* Make a plot of a categorical/discrete feature and the outcome. Is there a relationship? How can we transform this categorical data into numerical features that can be useful for OLS?\n",
    "\n",
    "* Find the correlation coefficient between features and the outcome. Is there a strong relationship between the two? Can you find the correlation coefficient between different transformations of the feature and the outcome?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add any EDA code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5c: Defining Helper Function or Helper Variables\n",
    "\n",
    "**This question is not graded, but we suggest that you put all your helper functions below for readability and ease of testing.** Use this space below to define any additional helper functions you may use in your final model. These can be transformation functions you identified in the optional question above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define any additional helper functions or variables you need here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5d: Defining The Pipeline Function\n",
    "\n",
    "Just as in the guided model from the previous question, you should encapsulate as much of your workflow into functions as possible. Your job is to select better features and define your own feature engineering pipeline inside the function `feature_engine_final` in the following cell. Use of `.pipe` is not required, but you are welcome to incorporate it! **You must not change the parameters inside `feature_engine_final`. Do not edit the two lines at the end of the question cell below. They are helper functions that define a linear model, fit your data, and compute RMSE. If you do, you will receive no credit for this question.** \n",
    "\n",
    "- Any feature engineering techniques that involve referencing `Sale Price` (for example, removing outlying `Sale Price` values from the training data) should be performed under the condition `if not is_test_set:`.\n",
    "- All other feature engineering techniques should be applied to both the training and test sets. This means that you should perform them under the condition `else:`.\n",
    "- When `is_test_set` is `True`, your function should return only the design matrix, `X`.\n",
    "- When `is_test_set` is `False`, your function should return both the design matrix and the response variable `Y` (the `Log Sale Price` column).\n",
    "\n",
    "**Hints:**\n",
    "-  Some features may have missing values in the test set but not in the training/validation set. Make sure `feature_engine_final` handles missing values appropriately for each feature.\n",
    "- We have imported all feature engineering functions from Project A1 for you. You do not have access to the `feature_func.py` file with the function body and definitions, but they work as defined in Project A1. Feel free to use them as you see fit!\n",
    "- You may wish to consider removing outlying datapoints from the training set before fitting your model. You may not, however, remove any datapoints from the test set (after all, the CCAO could not simply \"refuse\" to make predictions for a particular house!)\n",
    "- As you finetune your model, you may unintentionally consume too much Datahub memory, causing your kernel to crash. See `q5a` for guidance on how to resolve this!!\n",
    "\n",
    "**Note:** If you run into any errors, the [Proj. A2 Common Mistakes](https://ds100.org/debugging-guide/projA2/projA2.html) section of the [Data 100 Debugging Guide](https://ds100.org/debugging-guide) may be a helpful resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please include all of your feature engineering processes inside this function.\n",
    "# Do not modify the parameters of this function.\n",
    "def feature_engine_final(data, is_test_set=False):\n",
    "    # Whenever you access 'Log Sale Price' or 'Sale Price', make sure to use the\n",
    "    # condition is_test_set like this:\n",
    "    if not is_test_set:\n",
    "        # Processing for the training set (i.e. not the test set)\n",
    "        # CAN involve references to sale price!\n",
    "        # CAN involve filtering certain rows or removing outliers\n",
    "        data['Log Sale Price'] = np.log(data['Sale Price'])\n",
    "        ...\n",
    "    else:\n",
    "        # Processing for the test set\n",
    "        # CANNOT involve references to sale price!\n",
    "        # CANNOT involve removing any rows\n",
    "        ...\n",
    "        \n",
    "    # Processing for both test and training set\n",
    "    # CANNOT involve references to sale price!\n",
    "    # CANNOT involve removing any rows\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    # Return predictors (X) and response (Y) variables separately\n",
    "    if is_test_set:\n",
    "        # Predictors \n",
    "        X = ...\n",
    "        return X\n",
    "    else:\n",
    "        # Predictors. Your X should not include Log Sale Price!\n",
    "        X = ...\n",
    "        # Response variable\n",
    "        Y = ...\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "# DO NOT EDIT THESE TWO LINES!\n",
    "check_rmse_threshold = run_linear_regression_test_optim(lm.LinearRegression(fit_intercept=True), feature_engine_final, 'cook_county_train.csv', None, False)\n",
    "print(\"Current training RMSE:\", check_rmse_threshold.loss)\n",
    "print(\"You can check your grade for your prediction as per the grading scheme outlined at the start of Question 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5e: Fit and Evaluate your Model\n",
    "\n",
    "**This question is not graded.** Use this space below to evaluate your models. Some ideas are listed below. \n",
    "\n",
    "**Note:** While we have a grader function that checks RMSE for you, it is best to define and create your own model object and fit on your data. This way, you have access to the model directly to help you evaluate/debug if needed. For this project, you should use a `sklearn` default `LinearRegression()` model with intercept term for grading purposes. Do not modify any hyperparameter in `LinearRegression()`, and focus on feature selection or hyperparameters of your own feature engineering function.\n",
    "\n",
    "It may also be helpful to calculate the RMSE directly as follows:\n",
    "\n",
    "$$RMSE = \\sqrt{\\dfrac{\\sum_{\\text{houses in the set}}(\\text{actual price for house} - \\text{predicted price for house})^2}{\\text{number of houses}}}$$\n",
    "\n",
    "A function that computes the RMSE is provided below. Feel free to use it if you would like calculate the RMSE for your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values.\n",
    "    Input:\n",
    "      predicted (1D array): Vector of predicted/fitted values\n",
    "      actual (1D array): Vector of actual values\n",
    "    Output:\n",
    "      A float, the RMSE value.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<details>\n",
    "    <summary>[<b>Click to Expand</b>] Hints: </summary>\n",
    "    \n",
    "Train set:\n",
    "\n",
    "* Check your RMSE. Is this a reasonable number? You may use our grading scheme as a reference. Keep in mind that training error is generally less than testing error. \n",
    "\n",
    "Test set:\n",
    "* Find the original data shape at the beginning of the notebook (in the provided assert statement). What should the output shape be?\n",
    "\n",
    "* Since test and training/validation sets come from the same population (recall that test and training/validation sets are a random split from larger data), we expect our test prediction to have a similar range as the validation data. Plot the observed training (Log) Sale Price and the predicted (Log) Sale Price. Are the ranges similar? Do you have any unreasonable extreme prediction that cannot be exponentiated?\n",
    "\n",
    "* We cannot compute test RMSE directly since we do not have the observed values. Perform cross-validation to estimate your test error. Recall that we are treating the validation set as unseen data.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this space to evaluate your model\n",
    "# if you reset your memory, you need to define the functions again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5f Submission\n",
    "\n",
    "Recall that the test set given to you in this assignment does not contain values for the true `Sale Price` of each house. You will be predicting `Log Sale Price` on the data stored in `cook_county_contest_test.csv`. To determine your model's RMSE on the test set, you will submit the predictions made by your model to Gradescope. There, we will run checks to see what your test RMSE is by considering (hidden) true values for the `Sale Price`. We will delog/exponentiate your prediction on Gradescope to compute RMSE and use this to score your model. Before submitting to Gradescope, make sure that your predicted values can all be delogged (i.e., if one of your `Log Sale Price` predictions is 60, it is too large; $e^{60}$ is too big!)\n",
    "\n",
    "Your score on this section will be determined by the grading scheme outlined at the start of Question 5. **Remember that you can only submit your test set predictions to Gradescope up to 4 times per day. Plan your time to ensure that you can adjust your model as necessary, and please test your model's performance using cross-validation before making any submissions.** For more on cross-validation, check [Lecture 16](https://ds100.org/sp24/lecture/lec16/). In particular, the [Lecture 16 notebook](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Fsp24-student&urlpath=lab%2Ftree%2Fsp24-student%2F%2Flecture%2Flec16%2Flec16.ipynb&branch=main) may be helpful here. You can also feel free to reference what you did in previous questions when creating training and validation sets and seeing how your model performs.\n",
    "\n",
    "To determine the error on the test set, please submit your predictions on the test set to the Gradescope assignment **Project A2 Test Set Predictions**. The CSV file to submit is generated below, and you should not modify the cell below. Simply download the CSV file, and submit it to the appropriate Gradescope assignment.\n",
    "\n",
    "**You will not receive credit for the test set predictions (i.e., up to 3 points) unless you submit to this assignment**!!\n",
    "\n",
    "**Note:** If you run into any errors, the [Proj. A2 Common Mistakes](https://ds100.org/debugging-guide/projA2/projA2.html) section of the [Data 100 Debugging Guide](https://ds100.org/debugging-guide) may be a helpful resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "Y_test_pred = run_linear_regression_test(lm.LinearRegression(fit_intercept=True), feature_engine_final, None, 'cook_county_train.csv', 'cook_county_contest_test.csv', \n",
    "                                         is_test = True, is_ranking = False, return_predictions = True\n",
    "                                         )\n",
    "\n",
    "# Construct and save the submission:\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": pd.read_csv('cook_county_contest_test.csv')['Unnamed: 0'], \n",
    "    \"Value\": Y_test_pred,\n",
    "}, columns=['Id', 'Value'])\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = \"submission_{}.csv\".format(timestamp)\n",
    "submission_df.to_csv(filename, index=False)\n",
    "\n",
    "#print('Created a CSV file: {}.'.format(\"submission_{}.csv\".format(timestamp)))\n",
    "display(HTML(\"Download your test prediction <a href='\" + filename + \"' download>here</a>.\"))\n",
    "print('You may now upload this CSV file to Gradescope for scoring.')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Scratch space to check if your prediction is reasonable. See 5e for hints. \n",
    "# We will not reset the submission count for mis-submission issues.\n",
    "submission_df[\"Value\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing your prediction model for home sale prices in Cook County! In the following section, we'll delve deeper into the implications of predictive modeling within the CCAO case study, especially because statistical modeling is how the CCAO valuates properties. \n",
    "\n",
    "Refer to [Lecture 15](https://ds100.org/sp24/lecture/lec15/) if you're having trouble getting started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 6: Exploring RMSE\n",
    "\n",
    "Let's delve a bit deeper into what RMSE means in the context of predicting house prices. We will go through different ways of visualizing the performance of the model you created and see how that ties into questions about property taxes. To this end, we'll create the `preds_df` `DataFrame` below that will prove useful for the later questions.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell below; no further action is needed\n",
    "train_df = pd.read_csv('cook_county_train.csv')\n",
    "X, Y_true = feature_engine_final(train_df)\n",
    "model = lm.LinearRegression(fit_intercept=True)\n",
    "model.fit(X, Y_true)\n",
    "Y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'True Log Sale Price' : Y_true, 'Predicted Log Sale Price' : Y_pred, \n",
    "                         'True Sale Price' : np.e**Y_true, 'Predicted Sale Price' : np.e**Y_pred})\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 6a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's examine how our model performs on two halves of our data: `cheap_df` which contains the rows of `preds_df` with prices below or equal to the median sale price, and `expensive_df` which has rows of `preds_df` with true sale prices above the median. Take a moment to understand what is happening in the cell below, as it will also prove useful in `q6b`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run the cell below to obtain the two subsets of data; no further action is needed.\n",
    "min_Y_true, max_Y_true = np.round(np.min(Y_true), 1) , np.round(np.max(Y_true), 1)\n",
    "median_Y_true = np.round(np.median(Y_true), 1)\n",
    "cheap_df = preds_df[(preds_df['True Log Sale Price'] >= min_Y_true) & (preds_df['True Log Sale Price'] <= median_Y_true)]\n",
    "expensive_df = preds_df[(preds_df['True Log Sale Price'] > median_Y_true) & (preds_df['True Log Sale Price'] <= max_Y_true)]\n",
    "\n",
    "print(f'\\nThe lower interval contains houses with true sale price ${np.round(np.e**min_Y_true)} to ${np.round(np.e**median_Y_true)}')\n",
    "print(f'The higher interval contains houses with true sale price ${np.round(np.e**median_Y_true)} to ${np.round(np.e**max_Y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Compute the RMSE of your model's predictions of `Sale Price` on each subset separately**, and assign those values to `rmse_cheap` and `rmse_expensive` respectively.\n",
    "\n",
    "Separately, we also want to understand whether the proportion of houses in each interval that the model overestimates the value of the actual `Sale Price`. To that end, **compute the proportion of predictions strictly greater than the corresponding true price in each subset**, and assign it to `prop_overest_cheap` and `prop_overest_expensive` respectively. For example, if we were working with a dataset of 3 houses where the actual `Log Sale Price`s were [10, 11, 12] and the model predictions were [5, 15, 13], then the proportion of houses with overestimated values would be 2/3.\n",
    "\n",
    "**Note:** When calculating `prop_overest_cheap` and `prop_overest_expensive`, you could use either `Log Sale Price` or `Sale Price`. Take a second to think through why this metric is unchanged under a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_cheap = ...\n",
    "rmse_expensive = ...\n",
    "\n",
    "prop_overest_cheap = ...\n",
    "prop_overest_expensive = ...\n",
    "\n",
    "print(f\"The RMSE for properties with log sale prices in the interval {(min_Y_true, median_Y_true)} is {np.round(rmse_cheap)}\")\n",
    "print(f\"The RMSE for properties with log sale prices in the interval {(median_Y_true, max_Y_true)} is {np.round(rmse_expensive)}\\n\")\n",
    "print(f\"The percentage of overestimated values for properties with log sale prices in the interval {(min_Y_true, median_Y_true)} is {np.round(100 * prop_overest_cheap, 2)}%\")\n",
    "print(f\"The percentage of overestimated values for properties with log sale prices in the interval {(median_Y_true, max_Y_true)} is {np.round(100 * prop_overest_expensive, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 6b\n",
    "\n",
    "The intervals we defined above were rather broad. Let's try and take a more fine-grained approach to understand how RMSE and proportion of houses overestimated vary across different intervals of `Log Sale Price`. Complete the functions `rmse_interval` and `prop_overest_interval` to allow us to compute the appropriate values for any given interval. Pay close attention to the function description, and feel free to reuse and modify the code you wrote in the previous part as needed.\n",
    "\n",
    "**Note:** The autograder tests provided for each of the functions are **not** comprehensive as the outputs of the function are highly dependent on your model. Make sure that the values you obtain are interpretable and that the plots that follow look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse_interval(df, start, end):\n",
    "    '''\n",
    "    Given a design matrix X and response vector Y, computes the RMSE for a subset of values \n",
    "    wherein the corresponding Log Sale Price lies in the interval [start, end].\n",
    "\n",
    "    Input: \n",
    "    df : pandas DataFrame with columns 'True Log Sale Price', \n",
    "        'Predicted Log Sale Price', 'True Sale Price', 'Predicted Sale Price'\n",
    "    start : A float specifying the start of the interval (inclusive)\n",
    "    end : A float specifying the end of the interval (inclusive)\n",
    "    '''\n",
    "\n",
    "    subset_df = ...\n",
    "\n",
    "    rmse_subset = ...\n",
    "    return rmse_subset\n",
    "    \n",
    "def prop_overest_interval(df, start, end):\n",
    "    '''\n",
    "    Given a DataFrame df, computes prop_overest for a subset of values \n",
    "    wherein the corresponding Log Sale Price lies in the interval [start, end].\n",
    "\n",
    "    Input: \n",
    "    df : pandas DataFrame with columns 'True Log Sale Price', \n",
    "        'Predicted Log Sale Price', 'True Sale Price', 'Predicted Sale Price'\n",
    "    start : A float specifying the start of the interval (inclusive)\n",
    "    end : A float specifying the end of the interval (inclusive)\n",
    "    '''\n",
    "    \n",
    "    subset_df = ...\n",
    "\n",
    "    # DO NOT MODIFY THESE TWO LINES\n",
    "    if subset_df.shape[0] == 0:\n",
    "        return -1\n",
    "\n",
    "    prop_subset = ...\n",
    "    return prop_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 6c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that you've defined these functions, let's put them to use and generate some interesting visualizations of how the RMSE and proportion of overestimated houses vary for different intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RMSE plot\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.subplot(1, 2, 1) \n",
    "rmses = []\n",
    "for i in np.arange(8, 14, 0.5):\n",
    "    rmses.append(rmse_interval(preds_df, i, i + 0.5))\n",
    "plt.bar(x = np.arange(8.25, 14.25, 0.5), height = rmses, edgecolor = 'black', width = 0.5)\n",
    "plt.title('RMSE Over Different Intervals\\n of Log Sale Price', fontsize = 10)\n",
    "plt.xlabel('Log Sale Price')\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# Overestimation plot  \n",
    "plt.subplot(1, 2, 2)\n",
    "props = []\n",
    "for i in np.arange(8, 14, 0.5):\n",
    "    props.append(prop_overest_interval(preds_df, i, i + 0.5) * 100) \n",
    "plt.bar(x = np.arange(8.25, 14.25, 0.5), height = props, edgecolor = 'black', width = 0.5)\n",
    "plt.title('Percentage of House Values Overestimated \\nover different intervals of Log Sale Price', fontsize = 10)\n",
    "plt.xlabel('Log Sale Price')\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.ylabel('Percentage of House Values\\n that were Overestimated (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Explicitly referencing **ONE** of the plots above (using `props` and `rmses`), explain whether the assessments your model predicts more closely aligns with scenario C or scenario D that we discussed back in `q1b`. Which of the two plots would be more useful in ascertaining whether the assessments tended to result in progressive or regressive taxation? Provide a brief explanation to support your choice of plot. For your reference, the scenarios are also shown below:\n",
    "\n",
    "    C. An assessment process that systematically overvalues inexpensive properties and undervalues expensive properties.  \n",
    "    D. An assessment process that systematically undervalues inexpensive properties and overvalues expensive properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 7: Evaluating the Model in Context\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## Question 7a\n",
    "\n",
    "When evaluating your model, we used RMSE. In the context of estimating the value of houses, what does the residual mean for an individual homeowner? How does it affect them in terms of property taxes? Discuss the cases where the residual is positive and negative separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "In the case of the Cook County Assessor’s Office, Chief Data Officer Rob Ross states that fair property tax rates are contingent on whether property values are assessed accurately —— that they’re valued at what they’re worth, relative to properties with similar characteristics. This implies that having a more accurate model results in fairer assessments. The goal of the property assessment process for the CCAO, then, is to be as accurate as possible. \n",
    "\n",
    "When the use of algorithms and statistical modeling has real-world consequences, we often refer to the idea of fairness as a measurement of how socially responsible our work is. Fairness is incredibly multifaceted: Is a fair model one that minimizes loss - one that generates accurate results? Is it one that utilizes \"unbiased\" data? Or is fairness a broader goal that takes historical contexts into account?\n",
    "\n",
    "These approaches to fairness are not mutually exclusive. If we look beyond error functions and technical measures of accuracy, we'd not only consider _individual_ cases of fairness but also what fairness —— and justice —— means to marginalized communities on a broader scale. We'd ask: What does it mean when homes in predominantly Black and Hispanic communities in Cook County are consistently overvalued, resulting in proportionally higher property taxes? When the white neighborhoods in Cook County are consistently undervalued, resulting in proportionally lower property taxes? \n",
    "\n",
    "Having \"accurate\" predictions doesn't necessarily address larger historical trends and inequities, and fairness in property assessments in taxes works beyond the CCAO's valuation model. Disassociating accurate predictions from a fair system is vital to approaching justice at multiple levels. Take Evanston, IL —— a suburb in Cook County —— as an example of housing equity beyond just improving a property valuation model: their City Council members [recently approved reparations for African American residents](https://www.usnews.com/news/health-news/articles/2021-03-23/chicago-suburb-approves-government-reparations-for-black-residents).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 7b\n",
    "\n",
    "Reflecting back on your exploration in Questions 6 and 7a, in your own words, what makes a model's predictions of property values for tax assessment purposes \"fair\"? \n",
    "\n",
    "This question is open-ended and part of your answer may depend on your specific model; we are looking for thoughtfulness and engagement with the material, not correctness. \n",
    "\n",
    "**Hint:** Some guiding questions to reflect on as you answer the question above: What is the relationship between RMSE, accuracy, and fairness as you have defined it? Is a model with a low RMSE necessarily accurate? Is a model with a low RMSE necessarily \"fair\"? Is there any difference between your answers to the previous two questions? And if so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Ayga and Beck congratulate you on finishing Project A2!\n",
    "\n",
    "<center><video controls src = \"aygabeck.mp4\" width = \"250\">animation</video></a></center>\n",
    "\n",
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://docs.google.com/forms/d/e/1FAIpQLSe0fBEJwt6aEfZxU3fh3llNk8rSWHj6Umq0km3wPqmFu0MlGA/viewform). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the Project A2 Coding assignment on Gradescope, Gradescope will automatically submit a PDF file with your written answers to the Project A2 Written assignment. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide.\n",
    "\n",
    "If there are issues with automatically generating the PDF, you can try downloading the notebook as a PDF by clicking on `File -> Save and Export Notebook As... -> PDF`. If that doesn't work either, you can manually take screenshots of your answers to the manually graded questions and submit those. \n",
    "\n",
    "**Please make sure you submit the following to the right assignments:**\n",
    "\n",
    "* **Project A2 Coding:** Submit the zip file generated by using the `grader.export()` cell provided below.\n",
    "* **Project A2 Written:** Gradescope will automatically submit the PDF from the zip file submitted earlier. You do not need to submit anything to this assignment yourself, but *please check that the submission went through properly and that all plots rendered correctly*.\n",
    "* **Project A2 Test Set Predictions:** Submit the CSV file generated in `q5f`.\n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements and that everything was generated and submitted correctly. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q1c.lower() in ['a', 'b', 'c']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> train.shape == (163833, 62)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> validation.shape == (40959, 62)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(train['Sale Price'].mean(), 244939.22668204817, atol=0.1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.allclose(validation.index[-5:], [153946, 117415, 9448, 188605, 3223])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(validation['Sale Price'].mean(), 246066.1821089382, atol=0.1)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (q3a in ['>=', '=', '<=']) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3a == '>='\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(X_train_m1, pd.core.frame.DataFrame) and isinstance(Y_train_m1, pd.core.series.Series) and isinstance(X_valid_m1, pd.core.frame.DataFrame) and isinstance(Y_valid_m1, pd.core.series.Series) and isinstance(X_train_m2, pd.core.frame.DataFrame) and isinstance(Y_train_m2, pd.core.series.Series) and isinstance(X_valid_m2, pd.core.frame.DataFrame) and isinstance(Y_valid_m2, pd.core.series.Series)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert len(m2_pipelines) == 5\n>>> assert log_transform in set([p[0] for p in m2_pipelines])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(Y_fitted_m1.max(), 17.528601849438104, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(Y_fitted_m2.max(), 15.614096224439168, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(Y_predicted_m1.max(), 15.540922864181525, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(Y_predicted_m2.max(), 15.02563963305767, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q4b.lower() in ['regressive', 'fair', 'progressive']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5d": {
     "name": "q5d",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> check_rmse_threshold(200000)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> check_rmse_threshold(240000)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> check_rmse_threshold(280000)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> check_rmse_threshold.signature == (feature_engine_final, 'cook_county_train.csv', None)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> rmse_cheap >= 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> rmse_expensive >= 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= prop_overest_cheap <= 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= prop_overest_expensive <= 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6b": {
     "name": "q6b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> prop_overest_interval(preds_df, 10, 14) >= 0 and prop_overest_interval(preds_df, 10, 14) <= 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> rmse_interval(preds_df, 10, 14) < 1000000000.0 and rmse_interval(preds_df, 10, 14) > 0\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
