{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab12.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "# Lab 12: Principal Component Analysis\n",
    "\n",
    "\n",
    "In the lecture, we discussed how Principal Component Analysis (PCA) can be used for dimensionality reduction. Specifically, given a high dimensional dataset, PCA allows us to:\n",
    "1. Understand the rank of the data. If $k$ principal components capture almost all of the variance, then the data is roughly rank $k$.\n",
    "2. Create 2D scatterplots of the data. Such plots are a rank 2 representation of our data and allow us to identify clusters of similar observations visually.\n",
    "\n",
    "In this lab, we will walk through two examples that use PCA: One involving a dataset of iris plants ([link](https://en.wikipedia.org/wiki/Iris_plant)) and another involving an artificial \"surfboard\" 3D dataset. You'll learn how to perform PCA using the `np.linalg` package (Part 1) and build a geometric intuition of PCA to help you understand its strengths (Part 2).\n",
    "\n",
    "\n",
    "To receive credit for a lab, answer all questions correctly and submit before the deadline.\n",
    "\n",
    "**The on-time deadline is Tuesday, April 23, 11:59 PM**. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to late-night requests for assistance (TAs need to sleep, after all!). **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to contact staff for submission support.\n",
    "\n",
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note:** The recording was partially recorded in Spring 2022, when this lab was labeled Lab 11. In addition, questions 5 and 6 in this lab correspond to Questions 4 and 5 in the recording. There may be minor inconsistencies due to slightly different variable names (e.g. `s1` instead of `s`) but the content is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"gXZ9BJN6zB8\", list = 'PLQCcNQgUcDfqC3hTWqqEErLjHaPq0Yk3A', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Guide\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, common SQL errors, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook; no further action is needed\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight') # Use plt.style.available to see more styles\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 1: The Iris Dataset\n",
    "\n",
    "To begin, run the following cell to load the dataset into this notebook. \n",
    "* `iris_features` will contain a `NumPy` array of 4 attributes for 150 different plants (shape $150 \\times 4$). \n",
    "* `iris_target` will contain the class of each plant. There are three classes of plants in the dataset: Iris-Setosa, Iris-Versicolour, and Iris-Virginica. The class names will be stored in `iris_target_names`.\n",
    "* `iris_feature_names` will be a list of 4 names, one for each attribute in `iris_features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up; no further action is needed.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris() # Loading the dataset\n",
    "\n",
    "# Unpacking the data into arrays.\n",
    "iris_features = iris_data['data']\n",
    "iris_target = iris_data['target']\n",
    "iris_feature_names = iris_data['feature_names']\n",
    "iris_target_names = iris_data['target_names']\n",
    "\n",
    "# Convert iris_target to string labels instead of int labels currently (0, 1, 2) for the classes.\n",
    "iris_target = iris_target_names[iris_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data by creating a scatter matrix of our iris features. To do this, we'll create 2D scatter plots for every possible pair of our four features. This should result in six total scatter plots in our scatter matrix with the classes labeled in distinct colors for each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.suptitle(\"Scatter Matrix of Iris Features\", fontsize=20)\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "for i in range(1, 4):\n",
    "    for j in range(i):\n",
    "        plot_index = 3*j + i\n",
    "        plt.subplot(3, 3, plot_index)\n",
    "        sns.scatterplot(x=iris_features[:, i],\n",
    "                        y=iris_features[:, j],\n",
    "                        hue=iris_target,\n",
    "                       legend=(plot_index == 1))\n",
    "        plt.xlabel(iris_feature_names[i])\n",
    "        plt.ylabel(iris_feature_names[j])\n",
    "        if plot_index == 1:\n",
    "            plt.legend().remove()\n",
    "\n",
    "# same legend for all subplots.\n",
    "fig.legend(loc='lower left') \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "To apply PCA, we will first need to center the data so that the mean of each feature is 0. We will also go further and create **standardized/normalized** features, where we scale features such that they are centered with a standard deviation of 1. (We'll explore simple centered data in Part 2.)\n",
    "\n",
    "Compute the column-wise mean of `iris_features` in the cell below and store it in `iris_mean`. Then, compute the column-wise standard deviation of `iris_features` and store it in `iris_std`. Each should be a `NumPy` array of 4 means/standard deviations, 1 for each feature. \n",
    "\n",
    "Finally, subtract `iris_mean` from `iris_features`, divide by `iris_std`, and save the result in `iris_standardized`.\n",
    "\n",
    "**Hints:** \n",
    "* You may find `np.mean` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)), `np.average` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.average.html)), and/or `np.std` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.std.html)) helpful. Pay attention to the `axis` argument. \n",
    "* If you are confused about how `NumPy` deals with arithmetic operations between arrays of different shapes, see this note about broadcasting [(link)](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for explanations/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_mean = ...\n",
    "iris_std = ...\n",
    "iris_standardized = ...\n",
    "iris_mean, iris_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b\n",
    "\n",
    "As you may recall from the lecture, PCA is a specific application of the Singular Value Decomposition (SVD) for matrices. In the following cell, let's use the `np.linalg.svd` function ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html)) to compute the SVD of our `iris_standardized` matrix. \n",
    "\n",
    "Store the left singular vectors $U$, singular values or diagonal elements of $\\Sigma$, and (transposed) right singular vectors $V^T$ in `u`, `s`, and `vt`, respectively. Set the `full_matrices` argument of `np.linalg.svd` to `False`. Note that `s` returned by `np.linalg.svd` is a 1D array containing only the diagonal elements of $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u, s, vt = ...\n",
    "print(f\"Dimensions of U: {u.shape}\")\n",
    "print(f\"1D List of diagonal elements of Sigma: {s}\")\n",
    "print(f\"Dimensions of V Transpose: {vt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c\n",
    "\n",
    "What can we learn from the singular values in `s`?\n",
    "\n",
    "To answer this question, let's take a step back and recap what we know about variance. For a given feature, say sepal length, the variance associated with it can be calculated as follows:\n",
    "\n",
    "$$ \\text{Var} (X_\\text{sepal length}) = \\frac{1}{n} \\sum_{i=1}^n (x_{i, \\text{sepal length}} - \\bar{x}_\\text{sepal length})^2 $$\n",
    "\n",
    "where $x_{i, \\text{sepal length}}$ refers to the sepal length of the $i$-th iris, and $\\bar{x}_\\text{sepal length}$ is the average sepal length across all $n$ irises.\n",
    "\n",
    "As discussed in lecture, we define the total variance of a dataset $X$ to be the sum of variances of each of the features. In this case, \n",
    "\n",
    "$$ \\text{Var} (X) = \\text{Var} (X_\\text{sepal length}) + \\text{Var} (X_\\text{sepal width})+ \\text{Var} (X_\\text{petal length}) + \\text{Var} (X_\\text{petal width})$$\n",
    "\n",
    "The singular values in `s` provide us with a convenient way by which we can understand the total variance in $X$. Specifically, the total variance in the data is also equal to the sum of the squares of the singular values divided by the number of data points, that is:\n",
    "\n",
    "$$\\text{Var}(X) = \\frac{\\sum_{i=1}^p{\\sigma_i^2}}{n} = \\sum_{i=1}^p \\frac{\\sigma_i^2}{n} = \\sum_{i=1}^p \\text{Variance captured by } i\\text{-th PC}$$\n",
    "\n",
    "where for data $X$ with $n$ datapoints and $p$ features, $\\sigma_i$ is the singular value corresponding to the $i$-th principal component, and $\\text{Var}(X)$ is the total variance of the data. \n",
    "\n",
    "Compute the total variance of our data below by summing the square of each singular value in `s` and dividing the result by the total number of data points. Store the result in the variable `iris_total_variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_total_variance = ...\n",
    "\n",
    "print(\"iris_total_variance: {:.3f} should approximately equal the sum of the feature variances: {:.3f}\"\n",
    "      .format(iris_total_variance, np.sum(np.var(iris_standardized, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `iris_total_variance` equals the sum of the standardized feature variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "Let's now use only the first two principal components to see what a 2D version of our iris data looks like.\n",
    "\n",
    "First, construct the 2D version of the iris data by multiplying our `iris_standardized` array with the first two right singular vectors in $V$. Because the first two right singular vectors are directions for the first two principal components, this will project the iris data down from a 4D subspace to a 2D subspace.\n",
    "\n",
    "**Hints:**\n",
    "* To matrix-multiply two `NumPy` arrays, use `@` or `np.dot`. In case you're interested, the matmul documentation[(link)](https://numpy.org/devdocs/reference/generated/numpy.matmul.html) contrasts the two methods.\n",
    "* Note that in Question 1b, you computed `vt` (SVD decomposition is $U\\Sigma V^T$). The first two right singular vectors in $V$ will be the two rows of `vt`, transposed [(documentation)](https://numpy.org/devdocs/reference/generated/numpy.ndarray.T.html#numpy.ndarray.T) to be column vectors instead of row vectors. \n",
    "* Since we want to obtain a 2D version of our iris dataset, the shape of `iris_2d` should be $150 \\times 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_2d = ...\n",
    "iris_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now, run the cell below to create the scatter plot of our 2D version of the iris data, `iris_2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "plt.figure(figsize = (7, 7))\n",
    "plt.title(\"PC2 vs. PC1 for Iris Data\")\n",
    "plt.xlabel(\"Iris PC1\")\n",
    "plt.ylabel(\"Iris PC2\")\n",
    "sns.scatterplot(x = iris_2d[:, 0], y = iris_2d[:, 1], hue = iris_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "What do you observe about the plot above? If you were given a point in the subspace defined by PC1 and PC2, how well would you be able to classify the point as one of the three Iris types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "### Question 2c\n",
    "\n",
    "What proportion of the total variance is accounted for when we project the iris data into two dimensions? Compute this quantity in the cell below by dividing the variance captured by the first two singular values (also known as component scores) in `s` by the `iris_total_variance` you calculated previously. Store the result in `iris_2d_variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_2d_variance = ...\n",
    "iris_2d_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the variance in the data is explained by the two-dimensional projection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Next, we will create a **scree plot** ([link](https://en.wikipedia.org/wiki/Scree_plot)) to visualize the weight of each principal component. In the cell below, create a scree plot by making a line plot of the component scores (variance captured by each principal component) vs. the principal component number (1st, 2nd, 3rd, or 4th). Your graph should look similar to the image below:\n",
    "\n",
    "**Hint:** Be sure to label your axes appropriately! You may find `plt.xticks()` ([documentation](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.xticks.html)) helpful for formatting.\n",
    "\n",
    "<img src=\"images/scree.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## [Tutorial] Biplots\n",
    "\n",
    "Finally, we will analyze the **biplot** ([link](https://en.wikipedia.org/wiki/Biplot)) to understand how each feature contributes to the first two principal components. We do this by plotting the **directions**, or rows of $V^T$, which indicate how a feature correlates with each respective principal component. \n",
    "\n",
    "Recall that the columns of $U\\Sigma$ are the principal components of $X$. Because $V^T$ is an orthonormal matrix:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "X &=& U\\Sigma V^T \\\\\n",
    "XV &=& U\\Sigma V^T V = U\\Sigma I\\\\\n",
    "XV &=& U\\Sigma\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The direction vector $\\vec{v}_1$ indicates the amount with which to scale each feature vector to construct the first principal component. For example, if we define the principal component as $(U\\Sigma)_1 = \\sigma_1\\vec{u}_1$ and $\\vec{v}_j$ as the $j$-th direction (and therefore the $j$-th row of $V^T$):\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "| & & | \\\\\n",
    "\\vec{x}_1 & \\cdots & \\vec{x}_d \\\\\n",
    "| & & | \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "| & | & \\\\\n",
    "\\vec{v}_1 & \\vec{v}_2 & \\cdots \\\\\n",
    "| & | & \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "| & \\\\\n",
    "\\sigma_1\\vec{u}_1 & \\cdots  \\\\\n",
    "| & \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** SVD decomposition is not unique. For example,  \n",
    "\\begin{eqnarray}\n",
    "X &=& U\\Sigma V^T \\\\\n",
    "&=& (-U) \\Sigma (-V)^T \\\\\n",
    "&=& \\tilde{U} \\Sigma \\tilde{U}^T \\\\\n",
    "& \\tilde{U} = -U, \n",
    "& \\tilde{U} = -V\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, $X = \\tilde{U} \\Sigma \\tilde{V}^T$ is another valid SVD decomposition, and $\\tilde{U} \\Sigma$ is another valid principal component of X. The singular vectors $\\tilde{U}$ have the opposite sign than $U$. We set the random seed here to ensure we can reproduce the same decomposition. \n",
    "\n",
    "\n",
    "Run the below cell to generate the biplot for the Iris dataset. Based on the principal components plotted in the above biplot, what can you say about how each feature contributes to PC1 and PC2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot a biplot; no further action is needed.\n",
    "import random\n",
    "random.seed(42)\n",
    "cp = sns.color_palette()[1:] # Skip blue\n",
    "\n",
    "plt.figure(figsize = (7, 7))\n",
    "\n",
    "# First plot each datapoint in terms of the first two principal components.\n",
    "sns.scatterplot(x = iris_2d[:, 0], y = iris_2d[:, 1]);\n",
    "\n",
    "# Next, plot the loadings for PC1 and PC2.\n",
    "dir1, dir2 = vt[0,:], vt[1,:]\n",
    "# Just plotting the 2 arrows corresponding to 'sepal_width' and 'petal_length' \n",
    "for i, feature in enumerate(['', 'sepal width', 'petal length', '']):\n",
    "    plt.arrow(0, 0,\n",
    "              dir1[i], dir2[i],\n",
    "              head_width=0.2, head_length=0.2, color=cp[i])\n",
    "    plt.text(dir1[i] * 1+0.1*random.random(), # jitter\n",
    "             dir2[i] * 1+0.1*random.random(), \n",
    "             feature, fontsize=18, color=cp[i],\n",
    "             backgroundcolor=(1,1,1,0.6))\n",
    "\n",
    "plt.title(\"Iris Biplot\")\n",
    "plt.xlabel(\"Iris PC1\")\n",
    "plt.ylabel(\"Iris PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "Based on the principal components plotted in the above biplot, fill in the blanks for each of the following statements:\n",
    "\n",
    "Q4a. `sepal width` looks to be ___________ with PC1.<br/>\n",
    "Q4b. `sepal width` looks to be ___________ with PC2.<br/>\n",
    "Q4c. `petal length` looks to be ___________ with PC1.<br/>\n",
    "Q4d. `petal length` looks to be ___________ with PC2.\n",
    "\n",
    "Note that we have displayed the arrows for all the features in the dataset, though you only need to look at the labeled arrows to answer the question (green - sepal width, red - petal length).\n",
    "\n",
    "You should assign each variable (e.g., `q4a`) to`'A'`, `'B'`, `'C'` corresponding to the below:\n",
    "\n",
    "A. positively correlated<br/>\n",
    "B. negatively correlated <br/>\n",
    "C. uncorrelated\n",
    "\n",
    "In some cases, it may be difficult to draw the line between positively/negatively correlated and uncorrelated. The autograder tests will accept all reasonable answers if it is ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4a = ...\n",
    "q4b = ...\n",
    "q4c = ...\n",
    "q4d = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: PCA on 3D Data\n",
    "\n",
    "**In Part 2, our goal is to see visually how PCA is simply the process of rotating the coordinate axes of our data.**\n",
    "\n",
    "The code below reads in a 3D dataset. We named the `DataFrame` `surfboard` because the data resembles a surfboard when plotted in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "surfboard = pd.read_csv(\"data/data3d.csv\")\n",
    "surfboard.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial] Visualize the Data\n",
    "\n",
    "The cell below will allow you to view the data as a 3D scatterplot. Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure.\n",
    "\n",
    "You should see that the data is an ellipsoid that looks roughly like a surfboard or a hashbrown patty ([link](https://www.google.com/search?q=hashbrown+patty&source=lnms&tbm=isch)). It is pretty long in one direction, pretty wide in another, and relatively thin along its third dimension. We can think of these as the \"length\", \"width\", and \"thickness\" of the surfboard data.\n",
    "\n",
    "Observe that the surfboard is not aligned with the x/y/z axes.\n",
    "\n",
    "If you get an error that your browser does not support webgl, you may need to restart your kernel and/or browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "fig = px.scatter_3d(surfboard, \n",
    "                    x='x', y='y', z='z', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], height = 500, width = 600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the Data (Colorized)**\n",
    "\n",
    "To give the figure a little more visual pop, the following cell does the same plot but assigns a pre-determined color value (that we've arbitrarily chosen) to each point. *These colors do not mean anything important*; they're simply there as a visual aid.\n",
    "\n",
    "You might find using the `colorize_surfboard_data` method useful later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the colorized version of the previous cell; no further action is needed.\n",
    "def colorize_surfboard_data(df):\n",
    "    colors = pd.read_csv(\"data/surfboard_colors.csv\", header = None).values\n",
    "    df_copy = df.copy()\n",
    "    df_copy.insert(loc = 3, column = \"color\", value = colors)\n",
    "    return df_copy\n",
    "    \n",
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard), \n",
    "                    x='x', y='y', z='z', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], \n",
    "                    color = \"color\", color_continuous_scale = 'RdBu', height = 500, width = 600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 5a\n",
    "\n",
    "In Part 1, we standardized the Iris data before performing SVD, i.e., we made features zero-mean and unit-variance. In this part, we'll try just **centering** our data so that each feature is zero-mean and variance is unchanged.\n",
    "\n",
    "Compute the column-wise mean of `surfboard` in the cell below and store the result in `surfboard_mean`. You can make `surfboard_mean` a `NumPy` array or a `Series`, whichever is more convenient. Regardless of your data type, `surfboard_mean` should have 3 means: 1 for each attribute, with the $x$ coordinate first, then $y$, then $z$.\n",
    "\n",
    "Then, subtract `surfboard_mean` from `surfboard`, and save the result in `surfboard_centered`. The order of the columns in `surfboard_centered` should be $x$, then $y$, then $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surfboard_mean = ...\n",
    "surfboard_centered = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5b\n",
    "\n",
    "In the following cell, compute the SVD of `surfboard_centered` as $U\\Sigma V^T$, and store the left singular vectors $U$, singular values / diagonal elements of $\\Sigma$, and (transposed) right singular vectors $V^T$ in `u2`, `s2`, and `vt2`, respectively.\n",
    "\n",
    "Your code should be very similar to Question 1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u2, s2, vt2 = ...\n",
    "u2, s2, vt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "### Question 5c: Total Variance\n",
    "\n",
    "In Question 1c, we considered standardized features (each with unit variance), whose total variance was simply the count of features. Now, we'll show that the same relationship holds between singular values `s` and the variance of our (unstandardized) data.\n",
    "\n",
    "In the cell below, compute the total variance as the sum of the squares of the singular values $\\sigma_i$ divided by the number of datapoints $n$. Here's that formula again from Question 1c:\n",
    "\n",
    "$$\\text{Var}(X) = \\frac{\\sum_{i=1}^p{\\sigma_i^2}}{n} = \\sum_{i=1}^p \\frac{\\sigma_i^2}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_variance_computed_from_singular_values = ...\n",
    "total_variance_computed_from_singular_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "Your `total_variance_computed_from_singular_values` result should be very close to the total variance of the original `surfboard` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result; no further action is needed.\n",
    "np.var(surfboard, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total variance of our dataset is given by the sum of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result; no further action is needed.\n",
    "total_variance_computed_from_surfboard = sum(np.var(surfboard, axis=0))\n",
    "total_variance_computed_from_surfboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The variances are the same for both `surfboard_centered` and `surfboard` (why?), so we show only one to avoid redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5d: Variance Explained by First Principal Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the cell below, set `variance_explained_by_1st_pc` to the proportion of the total variance explained by the 1st principal component. Your answer should be a number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variance_explained_by_1st_pc = ...\n",
    "variance_explained_by_1st_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We can also create a scree plot that shows the proportion of variance explained by all of our principal components, ordered from most to least. You already constructed a scree plot for the Iris data, so we'll leave the surfboard scree plot for you to do on your own time.\n",
    "\n",
    "Instead, let's try to visualize why PCA is simply a rotation of the coordinate axes (i.e., features) of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6: V as a Rotation Matrix\n",
    "\n",
    "In the lecture, we saw that the first column of $XV$ contained the first principal component values for each observation, the second column of $XV$ contained the second principal component values for each observation, and so forth.\n",
    "\n",
    "Let's name this matrix: $P = XV = U\\Sigma$ is sometimes known as the \"principal component matrix\".\n",
    "\n",
    "Compute the $P$ matrix for the surfboard dataset and store it in the variable `surfboard_pcs`.\n",
    "\n",
    "**Hint:** What does $X$ represent here: `surfboard` or `surfboard_centered`? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surfboard_pcs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### [Tutorial] Visualizing the Principal Component Matrix\n",
    "\n",
    "We can think of $P$ as an output of the PCA procedure. $P$ is a **rotation** of the data such that the data will now appear \"axis aligned\". Specifically, for a 3d dataset, if we plot PC1, PC2, and PC3 along the $x$, $y$, and $z$ axes of our plot, then the greatest amount of variation happens along the $x$-axis, the second greatest amount along the $y$-axis, and the smallest amount along the $z$-axis. \n",
    "\n",
    "To visualize this, run the cell below, showing our data projected onto the principal component space. Compare it with your original figure, and observe that the data is precisely the same—only it is now rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "surfboard_pcs = surfboard_pcs.rename(columns = {0: \"pc1\", 1: \"pc2\", 2: \"pc3\"})\n",
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard_pcs), \n",
    "                    x='pc1', y='pc2', z='pc3', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], \n",
    "                    color = 'color', color_continuous_scale = 'RdBu', height = 500, width = 600);\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a 2D scatter plot of our `surfboard` data. Note that the resulting is just the 3D plot as viewed from directly \"overhead\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "sns.scatterplot(data = colorize_surfboard_data(surfboard_pcs), \n",
    "                x = 'pc1', y = 'pc2', hue = \"color\", palette = \"RdBu\", legend = False)\n",
    "plt.xlim(-10, 10);\n",
    "plt.ylim(-10, 10);\n",
    "plt.title(\"Top-Down View of $P$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 2 Summary\n",
    "\n",
    "Above, we saw that the principal component matrix $P$ is simply the original data rotated in space so that it appears axis-aligned.\n",
    "\n",
    "Whenever we create a 2D scatter plot of only the first 2 columns of $P$, we are simply looking at the data from \"above\", i.e., so that the 3rd (or higher) PC is invisible to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Congratulations! You are finished with Lab 12!\n",
    "\n",
    "<img src='images/bunny.PNG' width=\"350px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://docs.google.com/forms/d/e/1FAIpQLSe0fBEJwt6aEfZxU3fh3llNk8rSWHj6Umq0km3wPqmFu0MlGA/viewform). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Submit this file to the Lab 12 assignment on Gradescope. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> iris_std.size == 4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> iris_mean.size == 4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(np.isclose(iris_mean, np.array([5.84333333, 3.05733333, 3.758, 1.19933333])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(np.isclose(iris_std, np.array([0.82530129, 0.43441097, 1.75940407, 0.75969263])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(np.isclose(np.zeros(4), np.mean(iris_standardized, axis=0)))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(np.isclose(np.ones(4), np.std(iris_standardized, axis=0)))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> -2.56 < np.sum(iris_standardized[0]) < -2.5\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(s, np.array([20.92306556, 11.7091661, 4.69185798, 1.76273239])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> u.shape == (150, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> vt.shape == (4, 4)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(iris_total_variance, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(iris_total_variance, np.sum(np.var(iris_standardized, axis=0)))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> iris_2d.shape == (150, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> -2.75 < np.sum(iris_2d[0]) < -2.74\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(0, np.sum(iris_2d))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(iris_2d_variance, 0.9581320720000164)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> all([ans in ['A', 'B', 'C'] for ans in [q4a, q4b, q4c, q4d]])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4a == 'B' or q4a == 'C'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4b == 'B'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4c == 'A'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q4d == 'C'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(surfboard_mean) == 3\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> abs(surfboard_mean[0] - 0.0278) < 0.0001\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> abs(surfboard_mean[1] - 0.0332) < 0.0001\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> abs(surfboard_mean[2] - -0.0203) < 0.0001\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(surfboard_centered, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(np.mean(surfboard_centered, axis=0), np.array([0, 0, 0])))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> all(np.isclose([sum(u2[:, i] ** 2) for i in range(u2.shape[1])], [1, 1, 1]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose([sum(vt2[i, :] ** 2) for i in range(vt2.shape[1])], [1, 1, 1]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(s2) == 3\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(u2 @ np.diag(s2) @ vt2, surfboard_centered).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5c": {
     "name": "q5c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> abs(total_variance_computed_from_singular_values - sum(np.var(surfboard, axis=0))) < 1e-06\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5d": {
     "name": "q5d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0.5 < variance_explained_by_1st_pc < 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> abs(variance_explained_by_1st_pc - 0.8385) < 0.0001\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> surfboard_pcs.shape == (1000, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(np.mean(surfboard_pcs, axis=0), [0, 0, 0]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(abs(surfboard_pcs.loc[0]), [2.648, 0.851, 0.717], atol=0.001))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
