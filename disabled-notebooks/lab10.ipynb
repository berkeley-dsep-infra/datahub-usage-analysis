{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab10.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 10: Climate data and Inference\n",
    "\n",
    "The first part of the lab accompanies a lecture for Berkeley's Data 100 that covers the fundamental physical mechanisms behind global warming and analyzes CO2 and ocean temperature data. The second part explores correlation, bootstrapping, and confidence intervals of sample statistics.\n",
    "\n",
    "Climate Authors: Fernando PÃ©rez and [Dr. Chelle Gentemann](https://cgentemann.github.io).\n",
    "\n",
    "## Due Date\n",
    "This assignment is due on **April 4th, 2023, at 11:59PM PT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"5HbGE3JiBzQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Small style adjustments for more readable plots\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part I: Climate\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Tutorial: Loading the Mauna Loa CO2 data\n",
    "\n",
    "We start by loading the same dataset we used during the lecture, containing CO2 measurements in Mauna Loa, Hawaii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to load data from the local small copy available in the repo\n",
    "\n",
    "DATA_DIR = Path('./data')\n",
    "\n",
    "# If you want to run this on the Berkeley data hub, where we have a larger version of the data,\n",
    "# uncomment the below. But do not do that for submitting the Lab, as the version that will\n",
    "# run on the grader needs to use the path above and only has enough data to grade the Lab.\n",
    "#DATA_DIR = Path.home()/Path('shared/climate-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CO2 data from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_file = DATA_DIR / \"monthly_in_situ_co2_mlo_cleaned.csv\"\n",
    "data = pd.read_csv(co2_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values, which are stored as -99.99.\n",
    "data = pd.read_csv(co2_file, na_values=-99.99).dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the data\n",
    "plt.plot(\"fraction_date\", \"c02\", data=data, label=\"original\")\n",
    "plt.plot(\"fraction_date\", \"data_adjusted_seasonally_fit\", data=data, label=\"season adjusted\");\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"c02\")\n",
    "plt.legend()\n",
    "plt.title(\"c02 Overtime\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "# Tutorial: Exploring the annual anomaly\n",
    "\n",
    "Notice how in the plot, there seems to be oscilliations within each year (changing by the months/seasons), but the overall trend of these oscillations appears to be going upward. We are going to try to understand the annual variability on top of the underlying growing trend, and see **whether that variability within a given year is itself changing over time**.\n",
    "\n",
    "The figure above shows an annual cycle, alongside with perhaps some variability in it. As we saw in lecture (recall the super computer movie in talk with cities release of c02 strongest in winter), plants take up c02 in (northern) spring/summer then release in fall/winter --- so the release is getting stronger. \n",
    "\n",
    "The annual cycles look a bit like waves. Recall that the **amplitude** of a wave is the height of its peak. Let's try to estimate the increase in amplitude of annual cycle over time, in years.\n",
    "\n",
    "In the lecture notebook, we created the following figure, based on a quick and simple `groupby` operation and removal of the **annual mean**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "# Calculate the annual cycle using groupby\n",
    "annual = data.groupby(data.month).mean()\n",
    "# Calculate the anomaly\n",
    "anomaly = annual - annual.mean()\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(\"fraction_date\", \"data_filled\", \"r.\", data=data)\n",
    "ax.plot(\"fraction_date\", \"data_adjusted_seasonally_fit\", data=data)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"CO2 fraction in dry air (ppm)\")\n",
    "ax.set_title(\"Monthly Mean CO2\")\n",
    "ax.grid(False)\n",
    "\n",
    "axin1 = ax.inset_axes([0.1, 0.5, 0.35, 0.35])\n",
    "axin1.plot(anomaly.c02, \"b\")\n",
    "axin1.plot(anomaly.c02, \"r.\")\n",
    "axin1.set_title(\"Seasonal Anomaly\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you look closely, that figure isn't quite the same as the one shown in slide 9 of the lecture:\n",
    "\n",
    "<img src=\"images/annual-anomaly-orig.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by trying to understand the monthly data. The following shows us the data for all the years, by month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter('month', 'c02', data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately it's hard to see what's actually going on here. Let's explore this analysis further.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1\n",
    "\n",
    "Recreate the following figure that shows the monthly cycle for all the years in the dataset:\n",
    "\n",
    "**Hint:** Use `sns.lineplot(...)` ([documentation](https://seaborn.pydata.org/generated/seaborn.lineplot.html))\n",
    "\n",
    "<img src=\"images/monthly-cycles-co2.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Next, in order to attempt to recreate the figure in the talk, we're going to find the monthly anomaly averaging across year. To find this anomaly, we will first be \"detrending\" our data: we will be taking our data for each year, and subtracting off the mean from that year to get a sense of the variability about the average for each year. After detrending, we'll average the monthly data across all years. In this question, you will be writing code to detrend the data. \n",
    "\n",
    "You should end up with the following data frame after calling applying detrend function (only the first five rows are shown):\n",
    "\n",
    "<img src=\"images/yearly-co2-anomaly-df.png\" width=\"200px\" />\n",
    "\n",
    "And the following data frame at the end of the cell (only the first five rows are shown):\n",
    "\n",
    "<img src=\"images/monthly-co2-anomaly-df.png\" width=\"150px\" />\n",
    "\n",
    "\n",
    "**Hint:** `groupby.apply` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.apply.html)) takes in a subframe, perfrom computation within the subframe, and return a subframe for each group. You should return `\"year\"`, `\"month\"`, and the mean-centered `\"c02\"` for each year. One possible solution involves `pd.concat(..., axis='columns')` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)), take a look at the documentation on how to concatenate multiple columns into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detrend(df):\n",
    "    detrended_xarr = ...\n",
    "    ...\n",
    "\n",
    "    return detrended_xarr\n",
    "\n",
    "c02anomaly = data.groupby('year').apply(detrend)\n",
    "display(c02anomaly)\n",
    "monthly_anomaly = c02anomaly.groupby('month').mean()[['c02']]\n",
    "monthly_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "\n",
    "Instead of connected the data points with peice-wise linear line, we will connect the datapoints using cubic spline to smooth the curve. Cubic splines are a very useful method to interpolate data between points (i.e., to draw a function connecting multiple points so we get a better idea of what the function may look like in between points) using a smooth curve. Recreate the following figure, which is much closer to the one in the lecture:\n",
    "\n",
    "**Hints:**  Use `CubicSpline` from the package `scipy.interpolate` to create the fitted values bewteen the observed datapoints. You can find the documentation for `CubicSpline` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html). As an extra hint on using `CubicSpline`, its implementation is quite similar to any `sklearn` function you've already used. If you're curious about cubic spline: it is a piece-wise cubic function that not only passes through all datapoints but also has continuous first and second derivatives (more [here](https://pythonnumericalmethods.berkeley.edu/notebooks/chapter17.03-Cubic-Spline-Interpolation.html)).\n",
    "\n",
    "<img src=\"images/annual-anomaly-new.png\" width=\"600x\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# We use plt.subplots to create multiple plots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the main figure\n",
    "ax.plot(\"fraction_date\", \"data_filled\", \"r.\", data=data)\n",
    "ax.plot(\"fraction_date\", \"data_adjusted_seasonally_fit\", data=data)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"CO2 fraction in dry air (ppm)\")\n",
    "ax.set_title(\"Monthly Mean CO2\")\n",
    "ax.grid(False)\n",
    "\n",
    "# Fit a CubicSpline on the original data\n",
    "interp_model = ...\n",
    "# Define the points that we want to interpolate on\n",
    "mon_smooth = ...\n",
    "# `Predict` aka interpolate on these points\n",
    "c02_smooth = ...\n",
    "\n",
    "# Plot the subfigure on the top left corner\n",
    "axin1 = ax.inset_axes([0.1, 0.5, 0.35, 0.35])\n",
    "# Plot the original data as red dots \"r.\"\n",
    "axin1.plot(monthly_anomaly.index, monthly_anomaly[\"c02\"], \"r.\")\n",
    "# Plot the interpolated values as a blue line\n",
    "axin1.plot(mon_smooth, c02_smooth, \"b\")\n",
    "axin1.axhline(color='black')\n",
    "axin1.set_xticks([1, 4, 7, 10])\n",
    "axin1.set_xticklabels(['Jan', 'Apr', 'Jul', 'Oct'])\n",
    "axin1.set_title(\"Seasonal Anomaly\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: Inference for the Population Correlation\n",
    "\n",
    "In the previous lab we explored some properties of random variable and identity a biased estimator. For these parts, we assumed we had access to the population data. We simulate samples from the **true population** and calculate mean and varaince of these sample statistics. In practice, however, we only have access to one sample (and therefore one value of our estimator); we will explore this next.\n",
    "\n",
    "\n",
    "We define **population correlation** as the expected product of *standardized* deviations from expectation: \n",
    "\n",
    "$$r(X, Y) =  \\mathbb{E} \\left[\\left(\\frac{X - \\mathbb{E}[X]}{\\text{SD}(X)} \\right) \\left(\\frac{Y - \\mathbb{E}[Y]}{\\text{SD}(Y)}\\right)\\right]$$\n",
    "\n",
    "Note that population correlation involves the population means $\\mathbb{E}[X]$ and $\\mathbb{E}[Y]$ and the population standard deviations $\\text{SD}(X)$ and $\\text{SD}(Y)$. Correlation provides us with important information about the linear relationship between variables.\n",
    "\n",
    "In this part, we'll explore the `tips` dataset once more, and we will compute the sample correlation statistic of two features: **total bill** and **party size**. We will then explore how the sample correlation estimates the true population correlation parameter.\n",
    "\n",
    "The below cell assigns `data` to our single sample collected about customer tipping behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load tips data\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "data = tips[['total_bill','size']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "To estimate the population correlation, we'd like to use an estimator based on data from a simple random sample of our tips data set. For a sample $(X_1, Y_1), \\dots, (X_n, Y_n)$ generated IID from a population,  define the **sample correlation** as follows:\n",
    "\n",
    "$$\\frac{\\sum\\limits_{i=1}^n\\left(X_i-\\overline{X}\\right)\\left(Y_i-\\overline{Y}\\right)}{\\sqrt{\\sum\\limits_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}\\sqrt{\\sum\\limits_{i=1}^n \\left(Y_i - \\overline{Y}\\right)^2}}$$\n",
    "\n",
    "Note the similar structure to the true population correlation. If the $i$-th individual in our sample has \"total bill\" $X_i$ and \"party size\" $Y_i$, then $\\overline{X}, \\overline{Y}$ are the sample means of total bill and party size, respectively.\n",
    "\n",
    "Implement the `sample_correlation` function in the cell below to compute the sample correlation for `sample`, which has two columns: `total_bill` and `size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_correlation(sample):\n",
    "    \"\"\"\n",
    "    Compute sample correlation of x and y.\n",
    "    sample: A DataFrame of dimension (n, 2). The two columns are 'total_bill' and 'size'\n",
    "    \"\"\"\n",
    "    x, y = sample['total_bill'], sample['size']\n",
    "    x_bar = ...\n",
    "    y_bar = ...\n",
    "    n = ...\n",
    "    ...\n",
    "\n",
    "sample_correlation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Terminology\n",
    "\n",
    "Let the sample correlation of `data` be the estimator for the population correlation. In other words:\n",
    "\n",
    "* **Parameter**: Population correlation. Unknown, but fixed.\n",
    "* **Statistic**: Sample correlation. Dependent on the random sample we obtained.\n",
    "* **Estimator**: The sample correlation statistic `corr_est` is an estimator of the population correlation parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we infer about the population correlation given this estimate? Is it possible that the total bill and the party size are actually uncorrelated?\n",
    "\n",
    "We can perform **bootstrapped hypothesis testing** ([data 8 textbook](https://inferentialthinking.com/chapters/13/2/Bootstrap.html)): We cannot simulate samples from the true population, what if we resample from the sample instead? If the sample is representative of the true population, then the resample will also be similar to samples from the original population. Note that if your sample is not representative, boostrapping will also give you a bias result.\n",
    "\n",
    "The hypothesis are as follows:\n",
    "\n",
    "* **Null hypothesis**: Total bill and party size are uncorrelated; the population correlation is 0.\n",
    "* **Alternate hypothesis**: The population correlation is not 0.\n",
    "\n",
    "\n",
    "To test this hypothesis, we can bootstrap a $(1-p)$% confidence interval for the population correlation and check if 0 is in the interval. If 0 is in the interval, the data are consistent with the null hypothesis. If 0 is *not* in the interval, we reject the null hypothesis at the $p$% significance level. For more on the duality of the confidence interval and the p-value, see this [StackExchange discussion](https://stats.stackexchange.com/questions/179902/confidence-interval-p-value-duality-vs-frequentist-interpretation-of-cis). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5\n",
    "\n",
    "Implement the `ci_correlation` function in the cell below that returns a bootstrapped confidence interval at the `conf` level. Your bootstrap should resample the `sample` dataframe with replacement `n` times to construct `m` bootstrapped sample correlations using the `sample_correlation` function you implemented in Question 4.\n",
    "\n",
    "Then, assign `boot_ci` to the bootstrapped 95\\% confidence interval for the tips `data` sample.\n",
    "\n",
    "**Hint:** You may find `np.percentile` helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ci_correlation(sample, conf, m=5000):\n",
    "    \"\"\"Compute a confidence interval for an estimator.\n",
    "    sample: A DataFrame or Series\n",
    "    estimator: A function that maps a sample DataFrame to an estimate (number)\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    n = len(sample)\n",
    "    for j in range(m):\n",
    "        resample = ...\n",
    "        ...\n",
    "    lower = ...\n",
    "    upper = ...\n",
    "    return (lower, upper)\n",
    "\n",
    "boot_ci = ...\n",
    "boot_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6\n",
    "Now that we have the bootstrapped 95% confidence interval of the parameter based on a single sample of size 244, let's determine what we can conclude about our population correlation.\n",
    "\n",
    "Fill in the blanks for the sentence:\n",
    "\n",
    "> By bootstrapping our sample `data`, our estimate of the population correlation is ________ with a ___ % confidence interval of ________."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 7\n",
    "\n",
    "In the cell below, interpret the statement from the previous question. Can we reject the null hypothesis at the 5% significance level? What can we infer about the relationship between total bill and party size?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "finish",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Congratulations! You are finished with Lab 10!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(list(monthly_anomaly['c02']),\n...                   [-0.6838350970017644, 0.07612096774193428, 0.8742380952380925,\n...                     2.240269841269833, 2.8641545138888853, 2.299296594982075, \n...                     0.792918871252205, -1.1950176366843068, -2.851366843033514, \n...                     -2.8573163082437323, -1.453271604938272, -0.15930335097002282]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(sample_correlation(data),\n...            scipy.stats.pearsonr(data['total_bill'], data['size'])[0])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0.50 <= boot_ci[0] <= 0.52\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0.67 <= boot_ci[1] <= 0.69\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
